{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on importe les librairies nécessaires\n",
    "<ul>\n",
    "    <li>`import tensorflow as tf` -> bibliothèque open-source de calcul numérique </li>\n",
    "    <li>`from tensorflow.keras import layers,models` -> interface haut niveau pour construire des réseaux de neurones</li>\n",
    "    <li>`from sklearn.model_selection import train_test_split` -> fonction qui permet de séparer les données en un ensemble d\\'entraînement et un ensemble de validation </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "utilisation de la fonction `image_dataset_from_directory` de TensorFlow pour charger les données d'entraînement à partir d'un répertoire contenant des images\n",
    "- `train_data`: le chemin vers le répertoire contenant les données d'entraînement.\n",
    "- `batch_size`: taille des batchs sert à entraîner le modèle. ici un batch = 32 images.\n",
    "- `image_size`: la taille de chaque image dans les données d'entraînement (224 x 224 pixels).\n",
    "- `validation_split`: le ratio de données à utiliser pour la validation. 20% des données d'entraînement seront utilisées pour la validation.\n",
    "- `seed`: la seed utilisée pour le shuffle. permet que les données sont mélangées de la même manière à chaque fois que le code est exécuté.\n",
    "- `subset`: le sous-ensemble de données à utiliser.\n",
    "- `shuffle`: si les données doivent être mélangées à chaque époque.Ici données doivent être mélangées avant chaque époque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 files belonging to 2 classes.\n",
      "Using 4173 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../train',  \n",
    "    batch_size=32,          \n",
    "    image_size=(224, 224),  \n",
    "    validation_split=0.2,   \n",
    "    seed=42,                \n",
    "    subset='training',      \n",
    "    shuffle=True            \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les données de validation sont utilisées pour évaluer la performance du modèle pendant l'entraînement. \n",
    "Le modèle est entraîné sur les données d'entraînement, puis il est évalué sur les données de validation à chaque époque. \n",
    "Cela permet de détecter \n",
    "    - si le modèle est en train de s'adapter trop aux données d'entraînement et ne généralise pas bien à de nouvelles données\n",
    "    - s'il ne capture pas toutes les informations pertinentes des données d'entraînement.\n",
    "    Les données de validation permettent donc de surveiller la performance du modèle pendant l'entraînement et de l'améliorer en conséquence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16 files belonging to 2 classes.\n",
      "Using 3 files for validation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../val',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224),\n",
    "    validation_split=0.2,\n",
    "    seed=42, \n",
    "    subset='validation',\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_data` est utilisé pour évaluer les performances du modèle sur des données inédites,\n",
    " c'est-à-dire des données que le modèle n'a pas vues lors de l'entraînement et de la validation. \n",
    " il est donc utiilisé pour tester sa capacité à produire des prédictions précises sur des données qu'il n'a jamais vues auparavant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 624 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    '../test',\n",
    "    batch_size=32,\n",
    "    image_size=(224, 224)\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `prefetch()` est utilisée pour améliorer les performances du modèle en préchargeant un certain nombre de données en mémoire avant leur utilisation par le modèle.\n",
    "\n",
    "ici le modèle préchargera 32 images à la fois en mémoire avant leur utilisation par le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.prefetch(buffer_size=32)\n",
    "val_data = val_data.prefetch(buffer_size=32)\n",
    "test_data = test_data.prefetch(buffer_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le modèle de réseau de neurones convolutionnels (CNN) sert à entraîner le modèle. \n",
    "Voici une explication ligne par ligne :\n",
    "\n",
    "- `models.Sequential()` crée un modèle séquentiel qui permet de définir les couches du réseau dans l'ordre.\n",
    "- `layers.experimental.preprocessing.Rescaling(1./255)` normalise les valeurs de pixel de l'image pour qu'elles soient comprises entre 0 et 1.\n",
    "- `layers.Conv2D(16, (3,3), activation='relu', input_shape=(224,224,3))` définit une couche de convolution avec 16 filtres de taille (3,3) et une fonction d'activation relu. \n",
    "- `input_shape=(224,224,3)` indique que l'entrée de la couche est une image RGB de taille 224x224.\n",
    "- `layers.MaxPooling2D((2,2))` sert à réduire la taille de l'image.\n",
    "\n",
    "En résumé, ce modèle CNN est conçu pour extraire des caractéristiques d'images et les utiliser pour classer des images en deux catégories. \n",
    " - La première couche normalise les pixels de l'image,\n",
    " -  suivie de trois couches de convolution et de max-pooling pour extraire des caractéristiques d'image, \n",
    " - puis une couche dense pour classifier l'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(224,224,3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sert à compiler le modèle et définir les paramètres de l'apprentissage. \n",
    "\n",
    "- `optimizer` : il s'agit de l'algorithme d'optimisation utilisé pour mettre à jour les poids du modèle lors de l'apprentissage.\n",
    "- `loss` : c'est la fonction de perte utilisée pour calculer l'erreur du modèle lors de l'apprentissage. \n",
    "- `metrics` : il s'agit de la métrique utilisée pour évaluer les performances du modèle.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " sert à entraîner le modèle sur les données d'entraînement (train_data) et la validation sera effectuée sur les données de validation (val_data). \n",
    " epochs=10 indique qu'on entraîne le modèle pendant 10 époques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "131/131 [==============================] - 230s 2s/step - loss: 0.2495 - accuracy: 0.8924 - val_loss: 0.2296 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "131/131 [==============================] - 198s 1s/step - loss: 0.1118 - accuracy: 0.9573 - val_loss: 0.1074 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "131/131 [==============================] - 193s 1s/step - loss: 0.0824 - accuracy: 0.9708 - val_loss: 0.0942 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "131/131 [==============================] - 195s 1s/step - loss: 0.0597 - accuracy: 0.9796 - val_loss: 0.3287 - val_accuracy: 0.6667\n",
      "Epoch 5/10\n",
      "131/131 [==============================] - 184s 1s/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 0.2470 - val_accuracy: 0.6667\n",
      "Epoch 6/10\n",
      "131/131 [==============================] - 161s 1s/step - loss: 0.0460 - accuracy: 0.9839 - val_loss: 0.0489 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "131/131 [==============================] - 105s 788ms/step - loss: 0.0382 - accuracy: 0.9859 - val_loss: 0.0477 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "131/131 [==============================] - 105s 795ms/step - loss: 0.0254 - accuracy: 0.9911 - val_loss: 0.2267 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "131/131 [==============================] - 184s 1s/step - loss: 0.0251 - accuracy: 0.9902 - val_loss: 0.0162 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "131/131 [==============================] - 174s 1s/step - loss: 0.0251 - accuracy: 0.9911 - val_loss: 0.0304 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, validation_data=val_data, epochs=10)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fonction qui retourne les valeurs de la fonction de perte et de la métrique choisie sur les données de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 11s 413ms/step - loss: 1.7205 - accuracy: 0.7516\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on affiche la valeur de l'accuracy sur les données de test (test_acc) pour évaluer la performance finale du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7516025900840759\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

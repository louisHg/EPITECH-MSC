{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdac6562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from queue import Queue, LifoQueue\n",
    "from copy import copy\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        #print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f85d2521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: 6\n",
      "Observation space: 500\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3', render_mode='human')\n",
    "init_state = env.reset()\n",
    "env.render()\n",
    "\n",
    "action_num = env.action_space.n\n",
    "state_num = env.observation_space.n\n",
    "\n",
    "print('Action space:', action_num)\n",
    "print('Observation space:', state_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62d98566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [(1.0, 428, -1, False)],\n",
       " 1: [(1.0, 228, -1, False)],\n",
       " 2: [(1.0, 348, -1, False)],\n",
       " 3: [(1.0, 328, -1, False)],\n",
       " 4: [(1.0, 328, -10, False)],\n",
       " 5: [(1.0, 328, -10, False)]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_id = 328\n",
    "env.s = state_id\n",
    "env.render()\n",
    "env.P[state_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6918c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, env, parent = None):\n",
    "        self.state = env\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.untried_actions = [action for action in range(action_num)]\n",
    "        self.visiting_times = 0\n",
    "        self.q = 0\n",
    "        self.is_done = False\n",
    "        self.observation = None\n",
    "        self.reward = 0\n",
    "        self.action = None\n",
    "\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.untried_actions) == 0\n",
    "\n",
    "    def is_terminal_node(self):\n",
    "        return self.is_done\n",
    "\n",
    "    def compute_mean_value(self):\n",
    "        if self.visiting_times == 0:\n",
    "            return 0\n",
    "        return self.q / self.visiting_times\n",
    "\n",
    "    def compute_score(self, scale = 10, max_score = 10e100):\n",
    "        if self.visiting_times == 0:\n",
    "            return max_score\n",
    "        parent_visiting_times = self.parent.visiting_times\n",
    "        ucb = 2 * np.sqrt(np.log(parent_visiting_times) / self.visiting_times)\n",
    "        result = self.compute_mean_value() + scale * ucb\n",
    "        return result\n",
    "\n",
    "    def best_child(self):\n",
    "        scores = [child.compute_score() for child in self.children]\n",
    "        child_index = np.argmax(scores)\n",
    "        return self.children[child_index]\n",
    "\n",
    "    def expand(self):\n",
    "        action = self.untried_actions.pop()\n",
    "        next_state = copy(self.state)\n",
    "        self.observation, self.reward, self.is_done,_, _ = next_state.step(action)\n",
    "        child_node = Node(next_state, parent = self)\n",
    "        child_node.action = action\n",
    "        self.children.append(child_node)\n",
    "        return child_node\n",
    "  \n",
    "    def rollout_policy(self, state):\n",
    "        return state.action_space.sample()\n",
    "  \n",
    "    def rollout(self, t_max = 10**8):\n",
    "        state = copy(self.state)\n",
    "        rollout_return = 0\n",
    "        gamma = 0.6\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = self.rollout_policy(state)\n",
    "            obs, reward, done, _, _ = state.step(action)\n",
    "            rollout_return += gamma * reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        return rollout_return\n",
    "\n",
    "    def backpropagate(self, child_value):\n",
    "        node_value = self.reward + child_value\n",
    "        self.q += node_value\n",
    "        self.visiting_times += 1\n",
    "        if self.parent:\n",
    "            return self.parent.backpropagate(node_value)\n",
    "\n",
    "\n",
    "class MonteCarloTreeSearch(object):\n",
    "    def __init__(self, node):\n",
    "        self.root = node\n",
    "\n",
    "    def best_action(self, simulations_number):\n",
    "        for _ in range(0, simulations_number):\n",
    "            v = self._tree_policy()\n",
    "            reward = v.rollout()\n",
    "            v.backpropagate(reward)\n",
    "        return self.root.best_child()\n",
    "\n",
    "    def _tree_policy(self):\n",
    "        current_node = self.root\n",
    "        while not current_node.is_terminal_node():\n",
    "            if not current_node.is_fully_expanded():\n",
    "                return current_node.expand()\n",
    "            else:\n",
    "                current_node = current_node.best_child()\n",
    "        return current_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a00a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "# env.render()\n",
    "\n",
    "n_simulation = 10**4\n",
    "root = Node(env)\n",
    "is_done = False\n",
    "total_reward, penalty, epochs = 0, 0, 0\n",
    "\n",
    "while not is_done:\n",
    "    env.render()\n",
    "    mcts = MonteCarloTreeSearch(root)\n",
    "    best_child = mcts.best_action(n_simulation)\n",
    "    new_state, reward, is_done, info, _ = env.step(best_child.action)\n",
    "    total_reward += reward\n",
    "    if reward == -10:\n",
    "        penalty += 1\n",
    "    epochs += 1\n",
    "    root = best_child\n",
    "\n",
    "# env.render()\n",
    "print('Timesteps taken:', epochs)\n",
    "print('Penalty:', penalty)\n",
    "print('total_reward:', total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd392021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
